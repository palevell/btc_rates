#!/home/patrick/Projects/btc_rates/.venv/bin/python
# -*- coding: utf-8 -*-
# btc_rates4 - Friday, December 6, 2024
""" Retrieve Bitcoin prices using the yfinance library """
__version__ = "0.1.0-dev96"

import os
import sys
import time
from datetime import datetime, timedelta
from glob import glob
from os.path import exists, getmtime, join, lexists, realpath
from pathlib import Path
from random import randint, uniform

import click
import exchange_calendars as xcals
import pandas as pd
import sqlalchemy as sa
import xdg
import yfinance as yf
from dateutil.parser import parse, ParserError
from environs import Env
from loguru import logger
from sqlalchemy import create_engine, text
from sqlalchemy.exc import NoSuchTableError

_basedir = Path(__file__).resolve().parent.parent
__org__ = "LevellTech"
__project__ = _basedir.stem
__module__ = Path(__file__).resolve().stem

env = Env(expand_vars=True)
env.read_env()


def main():
	# Fetch last date in database
	tbl = _table_name
	symbols = env.list("YF_SYMBOLS", default=["BTC-CAD", "BTC-USD"])
	# symbols = ["BTC-CAD", "BTC-USD", "BCH-CAD", "BCH-USD", "BCH-BTC", "TSLA"]  # ToDo: environment variable (list)
	for i, symbol in enumerate(symbols):
		date_from = datetime(1970, 1, 1).date()
		date_thru = _run_dt.date()
		# Get date of last row in price table
		params = {"symbol": symbol,}
		sql = f"SELECT max(date_id) FROM {tbl} WHERE symbol = :symbol;"
		with engine.connect() as conn:
			last_date = conn.execute(text(sql), params).fetchone()[0]
			if last_date:
				date_from = last_date + timedelta(days=1)
		if date_from == _run_dt.date():
			logger.debug(f"Skipping {symbol} (data is current)")
			continue
		if i > 0:
			# Delay between calls to Yahoo Finance
			time.sleep(uniform(2.0, 4.0))
		logger.info(f"Fetching {symbol} data from {date_from} thru {date_thru} . . .")
		df = download_price_data(symbol, date_from, date_thru)
		result = df.to_sql(tbl, con=engine, schema=_db_schema, if_exists="append")
		# logger.debug(f"Result: {result}")
	# show_data()
	return


def init():
	started = time.strftime(_iso_datefmt, _run_localtime)
	logger.info(f"Run Start: {__module__} v{__version__} {started}")

	# Show configured directories
	show_paths()

	# Get list of table in database (for current schema)
	params = {"schema": _db_schema}
	sql = "SELECT relname FROM pg_stat_user_tables WHERE schemaname = :schema;"
	with engine.connect() as conn:
		rows = conn.execute(text(sql), params).fetchall()
		# table_names = [x[0] for x in rows]
		if _table_name not in [x[0] for x in rows]:
			# logger.error(f"Unable to find table '{_table_name}' in schema '{_db_schema}'")
			raise NoSuchTableError(f"{_db_schema}.{_table_name}")
	return


def eoj():
	stop_ts = time.time()
	stop_localtime = time.localtime(stop_ts)
	stop_gmtime = time.gmtime(stop_ts)
	duration = timedelta(seconds=(stop_ts - _run_ts))
	logger.info(f"Run Stop : {time.strftime(_iso_datefmt, stop_localtime)}  Duration: {duration}")
	return


def do_nothing():
	pass


def date_list(date_from: datetime.date, date_thru: datetime.date) -> list[datetime.date]:
	"""
	Provides a list of dates for the specified range
	:param date_from: start of date range
	:param date_thru: end of date range
	:return: list of dates
	"""
	return [date_from + i * timedelta(days=1) for i in range(1, (date_thru - date_from).days + 1)]


def download_price_data(symbol: str, date_from: datetime.date, date_thru: datetime.date) -> pd.DataFrame:
	"""
	Yahoo Finance market codes are garbage
	:param symbol: trading symbol (ie. TSLA, BTC-CAD, T)
	:param date_from: beginning of date range
	:param date_thru: end of date range
	:return: Pandas DataFrame containing price data
	"""
	df = pd.DataFrame()
	# Were the markets open for the specified dates?
	yf_xchg = yf.Ticker(symbol).info["exchange"]
	was_open = True
	if yf_xchg != "CCC":  # Cryptocurrency trading
		for dt in date_list(date_from, (date_thru - timedelta(days=1))):
			xcal = xcals.get_calendar("XNAS")  # NASDAQ
			was_open = xcal.is_session(dt)
			if was_open:
				break
		logger.debug(f"Did {symbol} trade from {date_from} thru {date_thru}? {was_open}")
	if was_open:
		df = yf.download(symbol, start=date_from, end=date_thru, interval="1d", multi_level_index=False, progress=False)
		if not df.empty:
			# Add column for symbol
			df.loc[:, "symbol"] = symbol
			# Rename index & columns
			df.index.name = "date_id"
			df.rename(columns={"Close": "price", }, inplace=True)
			# Only keep symbol and closing price
			df = df[["symbol", "price"]]
		else:
			raise Warning("Empty DataFrame--this was unexpected!")
	else:
		logger.debug(f"{symbol} markets were closed for specified dates")
		do_nothing()
	return df


def plot_prices():

	return


def show_data():
	tbl = _table_name
	# Show last three days of price data in the database
	columns = "date_id symbol price".split()
	cols = ",".join(columns)
	sql = "\n".join([
		f"SELECT {cols} FROM {tbl}",
		"WHERE date_id >= :date_from",
		"ORDER BY date_id DESC, symbol",
		"LIMIT 25;",
	])
	params = {"date_from": (_run_dt - timedelta(days=3)).date()}
	with engine.connect() as conn:
		result = conn.execute(text(sql), params)
	for date_id, symbol, price in result.fetchall():
		logger.info(f"{date_id} {symbol:10s} {price:12,.4f}")
	return


def show_paths():
	logger.debug(f"Organization  . . {__org__}")
	logger.debug(f"Project . . . . . {__project__}")
	logger.debug(f"Module  . . . . . {__module__}")
	logger.debug(f"Base Dir  . . . . {_basedir}")
	logger.debug(f"Cache Dir . . . . {_cache_dir}")
	logger.debug(f"Config Dir  . . . {_config_dir}")
	logger.debug(f"Data Dir  . . . . {_data_dir}")
	logger.debug(f"Log Dir . . . . . {_log_dir}")
	logger.debug(f"Runtime Dir . . . {_runtime_dir}")
	logger.debug(f"State Dir . . . . {_state_dir}")
	logger.debug(f"Env File  . . . . {env_file}")
	return


if __name__ == '__main__':
	_run_ts = time.time()
	_run_dt = datetime.fromtimestamp(_run_ts).astimezone()
	_run_localtime = time.localtime(_run_ts)
	_run_gmtime = time.gmtime(_run_ts)
	_run_ymd = time.strftime("%Y%m%d", _run_localtime)
	_run_hms = time.strftime("%H%M%S", _run_localtime)
	_run_ymdhms = f"{_run_ymd}_{_run_hms}"
	_iso_datefmt = "%Y-%m-%d %H:%M:%S%z"
	_fdate = _run_dt.strftime("%Y-%m")

	# Directories
	_cache_dir = xdg.xdg_cache_home() / __org__ / __project__
	_config_dir = xdg.xdg_config_home() / __org__ / __project__
	_data_dir = xdg.xdg_data_home() / __org__ / __project__
	_runtime_dir = xdg.xdg_runtime_dir() / __org__ / __project__
	_state_dir = xdg.xdg_state_home() / __org__ / __project__
	# Sub-Directories
	_log_dir = _state_dir / "log"

	# Environment
	env_file = _config_dir / "environment"
	# logger.debug(env_file)
	if env_file.exists():
		env.read_env(env_file, override=True, recurse=True)
	else:
		raise FileNotFoundError(env_file)
	_debug = env.bool("DEBUG", default=False)

	# Configure Logging (using Loguru)
	_logfile = _log_dir / f"{__module__}_{_fdate}.log"
	_errfile = _log_dir / f"{__module__}.err"

	logger.remove(0)
	logger.add(
		sys.stderr,
		backtrace=True,
		colorize=True,
		diagnose=True,
		format="<level>{level:8s} {function}:{line:03d}  {message}</level>",
		level=0,
	)
	logger.add(
		_logfile,
		colorize=False,
		compression="gz",
		format="<green>{time:%Y-%m-%d %H:%M:%S%z}</green> <level>{level:8s} {name}:{function}:{line:03d}  {message}</level>",
		rotation="10 MB",
		level="TRACE",
	)
	logger.add(
		_errfile,
		colorize=False,
		compression="gz",
		format="<green>{time:%Y-%m-%d %H:%M:%S%z}></green> <level>{level:8s} {name}:{function}:{line:03d}  {message}</level>",
		rotation="10 MB",
		level="ERROR",
	)
	# logger.add("debug.log", rotation="10 MB", level="DEBUG")
	# logger.add("info.log", rotation="10 MB", level="INFO")
	# logger.add("warning.log", rotation="10 MB", level="WARNING")
	# logger.add("critical.log", rotation="10 MB", level="CRITICAL")

	# Database
	_db_url = env("SQLALCHEMY_DATABASE_URI")
	_db_schema = env("PG_SCHEMA")
	_table_name = env("PG_DATA_TABLE", default="dt_yahoo_daily")
	engine = create_engine(_db_url, echo=_debug)

	@sa.event.listens_for(engine, "connect", insert=True)
	def set_search_path(dbapi_connection, connection_record):
		"""
		Set schema search path in database
		"""
		sql = f"SET SESSION search_path TO {_db_schema},public;"
		existing_autocommit = dbapi_connection.autocommit
		dbapi_connection.autocommit = True
		cursor = dbapi_connection.cursor()
		cursor.execute(sql)
		cursor.close()
		dbapi_connection.autocommit = existing_autocommit

	init()
	main()
	eoj()
