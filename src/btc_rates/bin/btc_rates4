#!/home/patrick/Projects/btc_rates/.venv/bin/python
# -*- coding: utf-8 -*-
# btc_rates4 - Friday, December 6, 2024
""" Retrieve Bitcoin prices using the yfinance library """
__version__ = "0.1.6-dev5"

import os
import psutil
import sys
import time
from datetime import datetime, timedelta
from glob import glob
from os.path import exists, getmtime, join, lexists, realpath
from pathlib import Path
from random import randint, shuffle, uniform

import click
import exchange_calendars as xcals
import pandas as pd
import requests
import sqlalchemy as sa
import xdg
import yfinance as yf
from dateutil.parser import parse, ParserError
from environs import Env
from loguru import logger
from requests import Session
from sqlalchemy import create_engine, text
from sqlalchemy.exc import NoSuchTableError

_basedir = Path(__file__).resolve().parent.parent
__org__ = "LevellTech"
__project__ = _basedir.stem
__module__ = Path(__file__).resolve().stem

env = Env(expand_vars=True)
env.read_env()


def main():
	# Fetch last date in database
	tbl = _table_name
	symbols = env.list("YF_SYMBOLS", default=["BTC-CAD", "BTC-USD"])
	shuffle(symbols)
	# yesterday = (_run_dt - timedelta(days=1)).date()
	csv_rows = []
	last_prices = {}
	# Create requests session for YFinance
	with requests.Session() as s:
		# headers = {"User-Agent": "brain-riffled-retract-typography's-dockets"}
		# s.headers = headers
		for i, symbol in enumerate(symbols):
			date_from = datetime(1970, 1, 1).date()
			date_thru = _run_dt.date()
			# Get date of last row in price table
			# sql = f"SELECT max(date_id) FROM {tbl} WHERE symbol = :symbol;"
			# Get last row in price table
			colums = "date_id price".split()
			cols = ",".join(colums)
			whr = "symbol = :symbol"
			ordr = "date_id DESC"
			params = {"symbol": symbol,}
			sql = f"SELECT {cols} FROM {tbl} WHERE {whr} ORDER BY {ordr} LIMIT 1;"
			with engine.connect() as conn:
				try:
					last_date, last_price = conn.execute(text(sql), params).fetchone()
				except TypeError as te:
					logger.warning(f"No data for {symbol}")
				except Exception as e:
					logger.exception(e)
					do_nothing()
				else:
					if last_date:
						date_from = last_date + timedelta(days=1)
						last_prices[symbol] = last_price
			if date_from == _run_dt.date():
				logger.debug(f"Skipping {symbol} (data is current)")
				continue
			if i > 0:
				# Delay between calls to Yahoo Finance
				sleeper(uniform(2.0, 4.0))
			# if date_from == yesterday:
			# 	msg = f"Processing {symbol} for yesterday . . ."
			# else:
			# 	msg = f"Processing {symbol} for {date_from} thru {yesterday} . . ."
			delay_seconds = uniform(_min_retry_seconds, _max_retry_seconds)
			for attempt in range(1, _max_retries + 1):
				try:
					if attempt > 1:
						msg = f"{i+1:3d}) Processing {symbol} (attempt {attempt}) . . ."
					else:
						msg = f"{i+1:3d}) Processing {symbol}  . . ."
					logger.info(msg)
					df = download_price_data(symbol, s, date_from, date_thru)
					# Add new price to database
					if df.shape[0]:
						df.to_sql(tbl, con=engine, schema=_db_schema, if_exists="append")
						lines = df.to_csv(header=False).splitlines()
						for line in lines:
							csv_rows.append(line.rstrip())
						do_nothing()
					break
				except Exception as e:
					if attempt < _max_retries:
						logger.warning(e)
						sleeper(uniform(_min_retry_seconds, _max_retry_seconds), attempt)
						delay_seconds += uniform(_min_retry_seconds, _max_retry_seconds)
					else:
						raise
	if csv_rows:
		seen_symbols = []
		# logger.info("*** New Data ***")
		# logger.info("     date     symbol   price")
		last_csv_date = ""
		for i, csv_row in enumerate(sorted(csv_rows)):
			csv_date, csv_symbol, csv_price = csv_row.split(",")
			if csv_date != last_csv_date:
				last_csv_date = csv_date
				y, m, d = [int(x) for x in csv_date.split("-")]
				dt = datetime(y, m, d).strftime("%A, %B %-d, %Y")
				# logger.info(f"*** New Data for {csv_date:10s} ***")
				logger.info(dt)
			csv_price = float(csv_price)
			if csv_symbol not in seen_symbols:
				seen_symbols.append(csv_symbol)
			if csv_price > 0.1:
				# msg = f"{csv_date:10s} {csv_symbol:8s} {csv_price:10,.2f}"
				msg = f"{i+1:3d}) {csv_symbol:8s} {csv_price:10,.2f}"
			else:
				# msg = f"{csv_date:10s} {csv_symbol:8s} {csv_price:12,.4f}"
				msg = f"{i+1:3d}) {csv_symbol:8s} {csv_price:12,.4f}"
			logger.info(msg)
		if len(symbols) > len(seen_symbols):
			unseen = []
			for s in symbols:
				if s not in seen_symbols:
					unseen.append(s)
			msg = "*** No new data for: " + ", ".join(sorted(unseen))
			logger.info(msg)
	return


def init():
	# global _headers
	# global _ua_dict
	started = time.strftime(_iso_datefmt, _run_localtime)
	logger.info(f"Run Start: {__module__} v{__version__} {started}")

	# Allow time for networking and XDG_RUNTIME_DIR to become available
	login_ts = psutil.users()[0].started
	wait_seconds = _run_ts - login_ts - 90  # 90 seconds
	if wait_seconds < 0:
		sleeper(abs(wait_seconds))

	# Set Firefox headers
	"""_headers = {
		# "Priority": "u=0, i",
		"Sec-Gpc": "1",
		# "Dnt": "1",
		"Sec-Fetch-User": "?1",
		"Sec-Fetch-Site": "none",
		"Sec-Fetch-Mode": "navigate",
		"Sec-Fetch-Dest": "document",
		# "Upgrade-Insecure-Requests": "1",
		# "Connection": "keep-alive",
		# "Accept-Encoding": "gzip, deflate, br, zstd",
		# "Accept-Language": "en-US,en;q=0.5",
		# "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
		"User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:135.0) Gecko/20100101 Firefox/135.0",
		# "Host": "patrickallan.online",
		# "Content-Length": "",
		# "Content-Type": "",
	}"""
	# Load user agents
	# _ua_dict = user_agents()

	# Get list of tables in database (for current schema)
	params = {"schema": _db_schema}
	sql = "SELECT relname FROM pg_stat_user_tables WHERE schemaname = :schema;"
	with engine.connect() as conn:
		rows = conn.execute(text(sql), params).fetchall()
		# table_names = [x[0] for x in rows]
		if _table_name not in [x[0] for x in rows]:
			# logger.error(f"Unable to find table '{_table_name}' in schema '{_db_schema}'")
			raise NoSuchTableError(f"{_db_schema}.{_table_name}")
	return


def eoj():
	stop_ts = time.time()
	stop_localtime = time.localtime(stop_ts)
	stop_gmtime = time.gmtime(stop_ts)
	duration = timedelta(seconds=(stop_ts - _run_ts))
	logger.info(f"Run Stop : {time.strftime(_iso_datefmt, stop_localtime)}  Duration: {duration}")
	return


def do_nothing():
	pass


def date_list(date_from: datetime.date, date_thru: datetime.date) -> list[datetime.date]:
	"""
	Provides a list of dates for the specified range
	:param date_from: start of date range
	:param date_thru: end of date range
	:return: list of dates
	"""
	if date_from == date_thru:
		dates = [date_from,]
	elif date_from < date_thru:
		dates = [date_from + i * timedelta(days=1) for i in range(1, (date_thru - date_from).days + 1)]
	else:
		raise ValueError(f"date_from > date_thru ({date_from} > {date_thru})")
	return dates


def download_price_data(symbol: str, session, date_from: datetime.date, date_thru: datetime.date) -> pd.DataFrame:
	"""
	Yahoo Finance market codes are garbage
	:param symbol: trading symbol (ie. TSLA, BTC-CAD, T)
	:param date_from: beginning of date range
	:param date_thru: end of date range
	:return: Pandas DataFrame containing price data
	"""
	df = pd.DataFrame()
	ticker = yf.Ticker(symbol, session=session)
	sleeper(uniform(2, 4))
	info_dict = ticker.get_info()
	sleeper(uniform(2, 4))
	# Were the markets open for the specified dates?
	yf_xchg = info_dict["exchange"]
	sleeper(uniform(2, 4))
	was_open = True
	if yf_xchg != "CCC":  # Cryptocurrency trading
		first_trade_epoch = info_dict["firstTradeDateMilliseconds"] / 1000
		first_trade_dt = datetime.fromtimestamp(first_trade_epoch).date()
		sleeper(uniform(2, 4))
		if date_from < first_trade_dt:
			logger.debug(f"First trade date: {first_trade_dt}")
			date_from = first_trade_dt
			do_nothing()
		for dt in date_list(date_from, (date_thru - timedelta(days=1))):
			xcal = xcals.get_calendar("XNAS")  # NASDAQ
			was_open = xcal.is_session(dt)
			if was_open:
				break
	if was_open:
		logger.debug(f"Fetching {symbol} data . . .")
		# max_retries = 10
		# delay_seconds = uniform(180, 300)
		delay_seconds = uniform(_min_retry_seconds, _max_retry_seconds)
		for retry in range(_max_retries):
			if retry > 0:
				logger.debug(f"Attempt #{retry+1}: yf.download({symbol}, start={date_from}, end={date_thru})")
				df = pd.DataFrame()
			df = yf.download(symbol, session=session, start=date_from, end=date_thru, auto_adjust=True, interval="1d", multi_level_index=False, progress=False)
			logger.debug(f"DataFrame size: {df.size}")
			if not df.empty:
				break
			else:
				delay_seconds += uniform(_min_retry_seconds, _max_retry_seconds)
				sleeper(delay_seconds, retry)
		else:
			logger.warning(f"No data for {symbol}")
		if not df.empty:
			df1 = df.loc[date_from:date_thru]
			extra_rows = df.shape[0] - df1.shape[0]
			if extra_rows:
				logger.warning(f"Yahoo! returned {extra_rows} extra row(s)")
				df = df1
			# Add column for symbol
			df.loc[:, "symbol"] = symbol
			# Rename index & columns
			df.index.name = "date_id"
			df.rename(columns={"Close": "price", }, inplace=True)
			# Only keep symbol and closing price
			df = df[["symbol", "price"]]
		else:
			# raise Warning("Empty DataFrame--this was unexpected!")
			do_nothing()
	else:
		logger.debug(f"{symbol} markets were closed.")
		do_nothing()
	return df


def plot_prices():

	return


def show_data(date_from: datetime.date, date_thru: datetime.date):
	tbl = _table_name
	# Show price data for the specified date range
	columns = "date_id symbol price".split()
	cols = ",".join(columns)
	# params = {"date_from": (_run_dt - timedelta(days=3)).date()}
	if date_from == date_thru:
		params = {"date_thru": date_thru,}
		whr = "date_id = :date_thru"
	else:
		params = {"date_from": date_from, "date_thru": date_thru,}
		whr = "date_id BETWEEN :date_from AND :date_thru"
	sql = "\n".join([
		f"SELECT {cols} FROM {tbl}",
		f"WHERE {whr}",
		"ORDER BY date_id DESC, symbol",
		"LIMIT 25;",
	])
	with engine.connect() as conn:
		result = conn.execute(text(sql), params)
	for date_id, symbol, price in result.fetchall():
		logger.info(f"{date_id} {symbol:10s} {price:12,.4f}")
	return


def show_paths():
	logger.debug(f"Organization  . . {__org__}")
	logger.debug(f"Project . . . . . {__project__}")
	logger.debug(f"Module  . . . . . {__module__}")
	logger.debug(f"Base Dir  . . . . {_basedir}")
	logger.debug(f"Cache Dir . . . . {_cache_dir}")
	logger.debug(f"Config Dir  . . . {_config_dir}")
	logger.debug(f"Data Dir  . . . . {_data_dir}")
	logger.debug(f"Log Dir . . . . . {_log_dir}")
	logger.debug(f"Runtime Dir . . . {_runtime_dir}")
	logger.debug(f"State Dir . . . . {_state_dir}")
	logger.debug(f"Env File  . . . . {env_file}")
	return


def sleeper(seconds: float, loop_count: int=0):
	now_dt = datetime.now().astimezone()
	future_dt = now_dt + timedelta(seconds=seconds)
	interval = future_dt - now_dt
	hms = str(interval).split(".")[0]
	msg = f"Sleeping {hms} until {future_dt.replace(microsecond=0)} . . ."
	if loop_count > 0:
		msg = f"Retry: {loop_count}; {msg}"
	logger.debug(msg)
	time.sleep(interval.total_seconds())
	return


def user_agents_old() -> dict:
	agents = {}
	for name in "default brave chrome curl falkon firefox opera yt-dlp".split():
		if name == "default":
			ua_filename = Path("~/.user_agent").expanduser()
		else:
			ua_filename = Path("~/.user_agents").expanduser() / name
		if not ua_filename.exists():
			raise FileNotFoundError(str(ua_filename))
		with open(ua_filename, "rt") as uafile:
			agents[name] = uafile.read().rstrip()
	return agents


if __name__ == '__main__':
	_run_ts = time.time()
	_run_dt = datetime.fromtimestamp(_run_ts).astimezone()
	_run_localtime = time.localtime(_run_ts)
	_run_gmtime = time.gmtime(_run_ts)
	_run_ymd = time.strftime("%Y%m%d", _run_localtime)
	_run_hms = time.strftime("%H%M%S", _run_localtime)
	_run_ymdhms = f"{_run_ymd}_{_run_hms}"
	_iso_datefmt = "%Y-%m-%d %H:%M:%S%z"
	_fdate = _run_dt.strftime("%Y-%m")

	# _headers = {}
	# _ua_dict = {}

	# Directories
	_cache_dir = xdg.xdg_cache_home() / __org__ / __project__
	_config_dir = xdg.xdg_config_home() / __org__ / __project__
	_data_dir = xdg.xdg_data_home() / __org__ / __project__
	_runtime_dir = xdg.xdg_runtime_dir() / __org__ / __project__
	_state_dir = xdg.xdg_state_home() / __org__ / __project__
	# Sub-Directories
	_log_dir = _state_dir / "log"

	# Environment
	env_file = _config_dir / "environment"
	# logger.debug(env_file)
	if env_file.exists():
		env.read_env(env_file, override=True, recurse=True)
	else:
		raise FileNotFoundError(env_file)
	_debug = env.bool("DEBUG", default=False)
	_max_retries = env.int("MAX_RETRIES", default=3)
	_min_api_seconds, _max_api_seconds = env.list("API_DELAY_RANGE", default=[1.5, 4.5], subcast=float)
	_min_retry_seconds, _max_retry_seconds = env.list("RETRY_DELAY_RANGE", default=[180, 300], subcast=float)

	# Configure Logging (using Loguru)
	_logfile = _log_dir / f"{__module__}_{_fdate}.log"
	_errfile = _log_dir / f"{__module__}.err"

	logger.remove(0)
	logger.add(
		sys.stderr,
		backtrace=True,
		colorize=True,
		diagnose=True,
		format="<level>{level:8s} {function}:{line:03d}  {message}</level>",
		level="INFO",
	)
	logger.add(
		_logfile,
		colorize=False,
		compression="gz",
		format="<green>{time:%Y-%m-%d %H:%M:%S%z}</green> <level>{level:8s} {name}:{function}:{line:03d}  {message}</level>",
		rotation="10 MB",
		level="TRACE",
	)
	logger.add(
		_errfile,
		colorize=False,
		compression="gz",
		format="<green>{time:%Y-%m-%d %H:%M:%S%z}></green> <level>{level:8s} {name}:{function}:{line:03d}  {message}</level>",
		rotation="10 MB",
		level="ERROR",
	)
	# logger.add("debug.log", rotation="10 MB", level="DEBUG")
	# logger.add("info.log", rotation="10 MB", level="INFO")
	# logger.add("warning.log", rotation="10 MB", level="WARNING")
	# logger.add("critical.log", rotation="10 MB", level="CRITICAL")

	# Database
	_db_url = env("SQLALCHEMY_DATABASE_URI")
	_db_schema = env("PG_SCHEMA")
	_table_name = env("PG_DATA_TABLE", default="dt_yahoo_daily")
	engine = create_engine(_db_url, echo=_debug)

	@sa.event.listens_for(engine, "connect", insert=True)
	def set_search_path(dbapi_connection, connection_record):
		"""
		Set schema search path in database
		"""
		sql = f"SET SESSION search_path TO {_db_schema},public;"
		existing_autocommit = dbapi_connection.autocommit
		dbapi_connection.autocommit = True
		cursor = dbapi_connection.cursor()
		cursor.execute(sql)
		cursor.close()
		dbapi_connection.autocommit = existing_autocommit


	# Workaround for Exchange Calendar default start date of 20 years ago
	xcals.exchange_calendar.GLOBAL_DEFAULT_START = pd.Timestamp("1970-01-01")

	init()
	main()
	eoj()
