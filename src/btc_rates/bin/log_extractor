#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# log_extractor - Thursday, November 14, 2024
""" Extract & analyze timestamps and prices from btc_rate logs """
__version__ = "0.1.0-dev40"

import builtins
import lzma
import os
import psycopg
import sqlite3 as lite
import sys
import time
from datetime import datetime, timedelta
from glob import glob
from os.path import exists, getmtime, join, lexists, realpath
from pathlib import Path
from time import sleep

from environs import Env
from sqlalchemy import create_engine, text
from xdg import XDG_CACHE_HOME, XDG_CONFIG_HOME, XDG_DATA_HOME, XDG_RUNTIME_DIR

_basedir = Path(__file__).resolve().parent.parent
__project__ = _basedir.stem
__module__ = Path(__file__).resolve().stem

env = Env(expand_vars=True)
env.read_env()


def main():
	extract_logs()
	return


def init():
	started = time.strftime(_iso_datefmt, _run_localtime)
	print(f"Run Start: {__project__}.{__module__} v{__version__} {started}")
	# print(f"Project . . . . . {__project__}")
	# print(f"Module  . . . . . {__module__}")
	# print(f"Base Dir  . . . . {_basedir}")
	# print(f"Cache Dir . . . . {_cache_dir}")
	# print(f"Config Dir  . . . {_config_dir}")
	# print(f"Data Dir  . . . . {_data_dir}")
	# print(f"Log Dir . . . . . {_log_dir}")
	# print(f"Runtime Dir . . . {_runtime_dir}")
	# for d in [_basedir, _cache_dir, _config_dir, _data_dir, _log_dir, _runtime_dir]:
	# 	if not exists(d):
	# 		print(f"Not Found: {d}")
	create_tables(_db_rebuild)
	return


def eoj():
	stop_ts = time.time()
	stop_localtime = time.localtime(stop_ts)
	stop_gmtime = time.gmtime(stop_ts)
	duration = timedelta(seconds=(stop_ts - _run_ts))
	print(
		f"Run Stop : {time.strftime(_iso_datefmt, stop_localtime)}  Duration: {duration}"
	)
	return


def do_nothing():
	pass


def create_tables(rebuild: bool=False):
	tables = ["dt_prices", "dt_daily"]
	for tbl in tables:
		filename = _basedir / "sqlite" / f"{tbl}.sql"
		with engine.connect() as conn, open(filename, "rt") as sqlfile:
			if rebuild:
				print(f"Dropping table {tbl} . . .")
				sql = f"DROP TABLE IF EXISTS {tbl};"
				conn.execute(text(sql))
			stmnts = sqlfile.read().split(";")
			for sql in stmnts:
				result = conn.execute(text(sql))
				if result.rowcount > 0:
					for row in result.fetchall():
						print(row)

	return


def extract_logs():
	last_date_id = ""
	last_ts = -1
	last_log_dict = last_log_entry()
	if last_log_dict:
		last_date_id = datetime.fromisoformat(last_log_entry()["date_id"])
		last_ts = last_date_id.timestamp()
	do_nothing()
	for entry in os.scandir(_log_dir):
		if getmtime(entry.path) < last_ts:
			continue
		# print(entry.path)
		log_dicts = parse_log(entry.path)
		columns = "date_id date_time cad usd".split()
		cols = ",".join(columns)
		sql = f"INSERT OR IGNORE INTO dt_prices({cols}) VALUES\n"
		values = []
		for ld in log_dicts:
			# print(f"{ld}")
			date_id = ld["date_id"]
			date_time = ld["when"]
			cad = ld["cad"]
			usd = ld["usd"]
			value = f"('{date_id}', '{date_time}', {cad}, {usd})"
			values.append(value)
		if values:
			vals = ",\n".join(values)
			sql += vals + ";\n"
			print(sql)
			print(f"Values Count . . . {len(values)}")
			with engine.connect() as conn:
				result = conn.execute(text(sql))
				conn.commit()
			print(f"Row Count  . . . . {result.rowcount}")
			sleep(0.1)
			do_nothing()
	return


def last_log_entry():
	log_dict = {}
	sql = "SELECT * FROM dt_prices ORDER BY date_time DESC LIMIT 1;"
	with engine.connect() as conn:
		try:
			id, date_id, date_time, cad, usd = conn.execute(text(sql)).fetchone()
			log_dict = {"id": id, "date_id": date_id, "date_time": date_time, "cad": cad, "usd": usd}
		except Exception as e:
			print(e, file=sys.stderr)
			do_nothing()
	return log_dict


def parse_log(filename: str | Path) -> list[dict]:
	"""
	:input: 2024-11-14 05:39:50-0500 btc_rates.get_prices INFO     CAD 127,455.71	USD 90,929.71
	:param filename:
	:return: log_dicts
	"""
	tag = "btc_rates.get_prices"
	log_dicts = []
	open = builtins.open
	if filename.endswith(".xz"):
		open = lzma.open
	with open(filename, "rt") as logfile:
		lines = [x.rstrip() for x in logfile.readlines() if tag in x]
	for i, line in enumerate(lines):
		# print(f"{i + 1:5,d}) {line}")
		parts = line.split()
		if parts[2] != tag:
			do_nothing()
		if len(parts) < 8:
			do_nothing()
		date_id = parts[0]
		when = datetime.fromisoformat(" ".join(parts[0:2]))
		if parts[4] == "CAD":
			cad = float(parts[5].replace(",", ""))
		else:
			continue
		if parts[6] == "USD":
			usd = float(parts[7].replace(",", ""))
		else:
			do_nothing()
		# print(f"       {when} CAD: {cad}  USD: {usd}")
		log_dict = {
			"when": when.isoformat(),
			"date_id": date_id,
			"cad": cad,
			"usd": usd
		}
		log_dicts.append(log_dict)
	return log_dicts


if __name__ == "__main__":
	_run_ts = time.time()
	_run_dt = datetime.fromtimestamp(_run_ts).astimezone()
	_run_localtime = time.localtime(_run_ts)
	_run_gmtime = time.gmtime(_run_ts)
	_run_ymd = time.strftime("%Y%m%d", _run_localtime)
	_run_hms = time.strftime("%H%M%S", _run_localtime)
	_run_ymdhms = f"{_run_ymd}_{_run_hms}"
	_iso_datefmt = "%Y-%m-%d %H:%M:%S%z"

	# Environment
	_db_rebuild = env.bool("DB_REBUILD", default=False)

	# Directories
	_cache_dir = Path(XDG_CACHE_HOME) / __project__
	_config_dir = Path(XDG_CONFIG_HOME) / __project__
	_data_dir = Path(XDG_DATA_HOME) / __project__
	_log_dir = Path(XDG_DATA_HOME).parent / "var" / "log" / __project__
	_runtime_dir = Path(XDG_RUNTIME_DIR) / __project__

	# Sub-Directories
	_log_dir2 = _cache_dir / "log"

	# Database
	_db_url = "sqlite:///" + str(_basedir / env("SQLITE_DB_NAME"))
	engine = create_engine(_db_url)

	# with engine.connect() as conn:
	# 	result = conn.execute(text("SELECT * FROM dim_date LIMIT 10;"))
	# 	if result.rowcount:
	# 		for row in result.fetchall():
	# 			print(row)

	init()
	main()
	eoj()
