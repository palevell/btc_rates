#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# summarize - Friday, November 15, 2024
""" Summarize Bitcoin Prices (ie. Statistics) - to be incorporated into price_extractor """
__version__ = "0.2.1-dev10"

import click
import os
import sys
import time
from datetime import datetime, timedelta
from glob import glob
from loguru import logger
from os.path import exists, getmtime, join, lexists, realpath
from pathlib import Path

import pandas as pd
import sqlalchemy as sa
from dateutil.parser import parse, ParserError
from dateutil.relativedelta import relativedelta
from environs import Env
from sqlalchemy import create_engine, text
from xdg import XDG_CACHE_HOME, XDG_CONFIG_HOME, XDG_DATA_HOME, XDG_RUNTIME_DIR

_basedir = Path(__file__).resolve().parent.parent
__project__ = _basedir.stem
__module__ = Path(__file__).resolve().stem

env = Env(expand_vars=True)
env.read_env()


def usage():
	logger.info(f"USAGE: {__module__} LOG_DATE")
	return


@click.command()
@click.argument("report_date", default="today")
@click.option("-f", "--force/--no-force", is_flag=True, default=True,
			  help="Force reports to be written (overwrite)",)
def main(report_date, force):
	logger.info(f"Report Date: {report_date}")
	yesterday = (_run_dt - timedelta(days=1)).date()
	if report_date.lower() in ["today", "yesterday"]:
		report_date = yesterday
	else:
		try:
			report_date = parse(report_date).date()
		except ParserError as e:
			logger.exception(e)
			raise
	if report_date > _run_dt.date():
		raise ValueError("Future dates aren't processed.")
	logger.debug(f"Processing Bitcoin price history for {report_date} . . .")
	build_reports(report_date, force)
	# for p in ["daily", "weekly", "monthly", "quarterly", "yearly"]:
	# 	try:
	# 		summarize(p, report_date)
	# 	except NotImplementedError as nie:
	# 		logger.warning(f"Not Implented: {nie}")
	return


def init():
	started = time.strftime(_iso_datefmt, _run_localtime)
	logger.info(f"Run Start: {__module__} v{__version__} {started}")
	return


def eoj():
	stop_ts = time.time()
	stop_localtime = time.localtime(stop_ts)
	stop_gmtime = time.gmtime(stop_ts)
	duration = timedelta(seconds=(stop_ts - _run_ts))
	logger.info(f"Run Stop : {time.strftime(_iso_datefmt, stop_localtime)}  Duration: {duration}")
	return


def do_nothing():
	pass


def build_table(table_name: str):
	table_name = table_name.lower()
	table_prefix = "dt_"
	if table_name not in ["daily", "weekly", "monthly", "quarterly", "yearly"]:
		raise ValueError(f"Unrecognized table_name: '{table_name}'")
	if table_name == "daily":
		# find dates in dt_price
		sql = """
		SELECT DISTINCT date_id
		FROM dt_price
		"""
	return


def build_query(db_column: str) -> str:
	"""
	Builds the SQL statement for building reports
	:param db_column: Date Dimension column to be used in WHERE clause
	:return: SQL statement
	"""
	logger.debug(f"db_column: {db_column}")
	columns = "date_id qty cad_open cad_lo cad_hi cad_close usd_open usd_lo usd_hi usd_close".split()
	cols = ", ".join(columns)
	query = "\n".join([
		f"SELECT {cols}",
		f"FROM daily_price",
		f"WHERE {db_column} = :report_date;",
	])
	return query


def build_reports(report_date: datetime.date, force: bool) -> None:
	logger.debug(f"Building reports for {report_date} . . .")
	reports = which_reports(report_date)
	if not reports:
		raise ValueError(f"No reports to generate for {report_date}.")
	msg = " ".join(reports)
	logger.debug(f"Reports to be generated for {report_date}: {msg}.")
	for report_type in reports:
		items = [
			f"Report Date: {report_date}",
			f" Type: {report_type.capitalize():7s}",
			f" DB Column: {reports[report_type]}",
		]
		logger.debug(" ".join(items))
		db_column = reports[report_type]
		if report_type == "daily":
			report_name = report_date.strftime("%A")
		elif report_type == "weekly":
			report_name = report_date.strftime("%Y-W%U")
		elif report_type == "monthly":
			report_name = report_date.strftime("%Y-%m-%B")
		elif report_type == "quarterly":
			# ToDo: Additional information from the Date Dimension is needed
			quarter = -1
			report_name = report_date.strftime("%Y-Q{quarter}")
		elif report_type == "yearly":
			report_name = report_date.strftime("%Y-W%U")
		else:
			raise ValueError(f"Unknown report type: {report_type}")
		params = {"report_date": str(report_date), }
		sql = build_query(db_column)
		logger.debug(sql)
		# sql2 = "SELECT COUNT(*) FROM dt_price;"
		# logger.debug(sql2)
		with engine.connect() as conn:
			results = conn.execute(text(sql), params).fetchone()
			row = dict(results._mapping)
			columns = row.keys()
			logger.debug("  ".join(columns))
			logger.debug("  ".join([str(x) for x in row.values()]))
			do_nothing()
	return


def currency_columns(currency_code: str, db_column: str) -> list[str]:
	"""

	:param currency_code: three-letter currency code (eg. CAD or USD)
	:return: list of currency columns
	"""
	cc = currency_code[:4].lower()
	"""
		MIN(cad) AS cad_lo, MAX(cad) AS cad_hi,
		(SELECT cad FROM dt_price WHERE date_id = '2024-11-25' ORDER BY date_id ASC) cad_open,
		(SELECT cad FROM dt_price WHERE date_id = '2024-11-25' ORDER BY date_id DESC) cad_close
	"""
	columns = [
		f"MIN({cc}) AS {cc}_lo",
		f"MAX({cc}) AS {cc}_hi",
		f"(SELECT {cc} FROM dt_price WHERE {db_column} = :report_date ORDER BY {db_column} ASC) {cc}_open",
		f"(SELECT {cc} FROM dt_price WHERE {db_column} = :report_date ORDER BY {db_column} DESC) {cc}_close",
	]
	return columns


# ToDo: Change 'period' to 'date' and check against period-thru-dates in date dimension
def summarize(period: str, report_date):
	src_table = None
	dst_table = None
	group_by = None
	thru_date = None
	if period == "daily":
		src_table = "dt_price"
		dst_table = "dt_daily"
		group_by = "date_id"
		thru_date = _run_dt.date() - timedelta(days=1)
		do_nothing()
	elif period == "weekly":
		src_table = "prices"
		dst_table = "dt_weekly"
		group_by = "week_id"
	elif period == "monthly":
		do_nothing()
	elif period == "quarterly":
		do_nothing()
	elif period == "yearly":
		do_nothing()
	else:
		raise ValueError(f"Unrecognized time period: '{period}'.  Aborting.")
	if not src_table:
		raise NotImplementedError(f"function summarize({period}, {report_date})")
	sql = f"SELECT * FROM {src_table} WHERE {group_by} <= '{thru_date}';"
	logger.debug(sql)
	# df = pd.read_sql(src_table, con=engine).groupby(group_by)[["cad","usd"]].agg(["first", "last", "min", "max"])
	df = pd.read_sql(sql, con=engine).groupby(group_by)[["cad","usd"]].agg(["first", "last", "min", "max"])

	# Ref: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/
	df.columns = ["_".join(x) for x in df.columns.ravel()]

	# df.to_sql("dt_daily", con=engine, if_exists="replace")
	df[("cad_prev")] = df[("cad_last")].shift(1)
	df[("cad_chg_amt")] = df[("cad_last")] - df[("cad_prev")]
	df[("cad_chg_pct")] = (df[("cad_last")] - df[("cad_prev")]) * 100.0 / df[("cad_prev")]

	df[("usd_prev")] = df[("usd_last")].shift(1)
	df[("usd_chg_amt")] = df[("usd_last")] - df[("usd_prev")]
	df[("usd_chg_pct")] = (df[("usd_last")] - df[("usd_prev")]) * 100.0 / df[("usd_prev")]

	logger.debug(df.tail(5))
	df.to_sql(dst_table, con=engine, if_exists="replace")
	return


def which_reports(report_date: datetime.date):
	reports = dict()

	# sql = f"SELECT * FROM dim_date WHERE date_id = :report_date;"
	sql = f"SELECT * FROM dim_date WHERE date_id = :report_date LIMIT 1;"
	params = {"report_date": report_date}
	rows = None
	with engine.connect() as conn:
		results = conn.execute(text(sql), params).fetchone()
		rows = dict(results._mapping)
		do_nothing()
	if rows:
		datediff = (_run_dt.date() - report_date).days
		if datediff <= 7:
			# Only generate daily reports for the last week
			reports.update({"daily": "date_id"})
		for period in ["week", "month", "quarter", "year"]:
			if period == "week" and datediff > 100:
				logger.warning(
					"Not generating weekly lists for dates more than three months ago"
				)
				continue
			elif period == "month" and datediff > 366:
				logger.warning(
					"Not generating monthly lists for dates more than a year ago"
				)
				continue
			col = f"{period}_thru"
			if rows[col] == report_date:
				reports.update({f"{period}ly": col})
	return reports


if __name__ == '__main__':
	_run_ts = time.time()
	_run_dt = datetime.fromtimestamp(_run_ts).astimezone()
	_run_localtime = time.localtime(_run_ts)
	_run_gmtime = time.gmtime(_run_ts)
	_run_ymd = time.strftime("%Y%m%d", _run_localtime)
	_run_hms = time.strftime("%H%M%S", _run_localtime)
	_run_ymdhms = f"{_run_ymd}_{_run_hms}"
	_iso_datefmt = "%Y-%m-%d %H:%M:%S%z"

	# Directories
	_cache_dir = Path(XDG_CACHE_HOME) / __project__
	_config_dir = Path(XDG_CONFIG_HOME) / __project__
	_data_dir = Path(XDG_DATA_HOME) / __project__
	_log_dir = Path(XDG_DATA_HOME).parent / "var" / "log" / __project__
	_runtime_dir = Path(XDG_RUNTIME_DIR) / __project__

	# Sub-Directories
	_log_dir2 = _cache_dir / "log"

	# Configure Logging (using Loguru)
	_logfile = _log_dir2 / f"{__module__}.log"
	_errfile = _log_dir2 / f"{__module__}.err"

	logger.remove(0)
	logger.add(
		sys.stderr,
		backtrace=True,
		colorize=True,
		diagnose=True,
		format="<level>{level:8s} {function}:{line:03d}  {message}</level>",
		level=0,
	)
	logger.add(
		_logfile,
		colorize=False,
		compression="gz",
		format="<green>{time}</green> <level>{level:8s} {name}:{function}:{line:03d}  {message}</level>",
		rotation="10 MB",
		level="TRACE",
	)
	logger.add(
		_errfile,
		colorize=False,
		compression="gz",
		format="<green>{time}></green> <level>{level:8s} {name}:{function}:{line:03d}  {message}</level>",
		rotation="10 MB",
		level="ERROR",
	)
	# logger.add("debug.log", rotation="10 MB", level="DEBUG")
	# logger.add("info.log", rotation="10 MB", level="INFO")
	# logger.add("warning.log", rotation="10 MB", level="WARNING")
	# logger.add("critical.log", rotation="10 MB", level="CRITICAL")


	# Database

	# Environment
	_debug = env.bool("DEBUG", default=False)
	_db_url = env("SQLALCHEMY_DATABASE_URI")
	_db_schema = env("DB_SCHEMA")
	engine = create_engine(_db_url, echo=_debug)

	@sa.event.listens_for(engine, "connect", insert=True)
	def set_search_path(dbapi_connection, connection_record):
		"""
		Set schema search path in database
		"""
		sql = f"SET SESSION search_path TO {_db_schema},public;"
		existing_autocommit = dbapi_connection.autocommit
		dbapi_connection.autocommit = True
		cursor = dbapi_connection.cursor()
		cursor.execute(sql)
		cursor.close()
		dbapi_connection.autocommit = existing_autocommit

	# if len(sys.argv) > 1:
	# 	init()
	# 	for i in range(3):
	# 		file_count = main(sys.argv[1:])
	# 		if file_count == 0:
	# 			break
	# 	eoj()
	# else:
	# 	usage()

	init()
	main()
	eoj()

"""
if __name__ == "__main__":
	_run_ts = time.time()
	_run_dt = datetime.fromtimestamp(_run_ts).astimezone()
	_run_localtime = time.localtime(_run_ts)
	_run_gmtime = time.gmtime(_run_ts)
	_run_ymd = time.strftime("%Y%m%d", _run_localtime)
	_run_hms = time.strftime("%H%M%S", _run_localtime)
	_run_ymdhms = f"{_run_ymd}_{_run_hms}"
	_iso_datefmt = "%Y-%m-%d %H:%M:%S%z"

	_log_dir = Path("~/.local/var/log").expanduser()
	_torrent_home = Path(f"~/Torrents").expanduser()
	_slug = randomchars(8)

	_tmpdir = Path(os.getenv("TMP"))

	# Configure Logging (using Loguru)
	_logfile = _log_dir / f"{__module__}.log"
	_errfile = _log_dir / f"{__module__}.err"
	logger.remove(0)
	logger.add(
		sys.stderr,
		backtrace=True,
		colorize=True,
		diagnose=True,
		format="<level>{level:8s} {function}:{line:03d}  {message}</level>",
		level=0,
	)
	logger.add(
		_logfile,
		colorize=False,
		compression="gz",
		format="<green>{time}</green> <level>{level:8s} {name}:{function}:{line:03d}  {message}</level>",
		rotation="10 MB",
		level="TRACE",
	)
	logger.add(
		_errfile,
		colorize=False,
		compression="gz",
		format="<green>{time}></green> <level>{level:8s} {name}:{function}:{line:03d}  {message}</level>",
		rotation="10 MB",
		level="ERROR",
	)
	# logger.add("debug.log", rotation="10 MB", level="DEBUG")
	# logger.add("info.log", rotation="10 MB", level="INFO")
	# logger.add("warning.log", rotation="10 MB", level="WARNING")
	# logger.add("critical.log", rotation="10 MB", level="CRITICAL")

	if len(sys.argv) > 1:
		init()
		for i in range(3):
			file_count = main(sys.argv[1:])
			if file_count == 0:
				break
		eoj()
	else:
		usage()
"""
