#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# price_extractor - Thursday, November 14, 2024
""" Extract & analyze timestamps and prices from btc_rate logs """
__version__ = "0.2.2-dev33"

import builtins
import lzma
import os
import sys
import time
from datetime import datetime, timedelta
from glob import glob
from os.path import exists, getmtime, join, lexists, realpath
from pathlib import Path
from time import sleep

import sqlalchemy as sa
from loguru import logger
from environs import Env
from sqlalchemy import create_engine, text
from subprocess import run
from xdg import XDG_CACHE_HOME, XDG_CONFIG_HOME, XDG_DATA_HOME, XDG_RUNTIME_DIR

_basedir = Path(__file__).resolve().parent.parent
__project__ = _basedir.stem
__module__ = Path(__file__).resolve().stem

env = Env(expand_vars=True)
env.read_env()


def main():
	get_remote_prices()
	extract_prices()
	return


def init():
	started = time.strftime(_iso_datefmt, _run_localtime)
	logger.info(f"Run Start: {__project__}.{__module__} v{__version__} {started}")
	# logger.debug(f"Project . . . . . {__project__}")
	# logger.debug(f"Module  . . . . . {__module__}")
	# logger.debug(f"Base Dir  . . . . {_basedir}")
	# logger.debug(f"Cache Dir . . . . {_cache_dir}")
	# logger.debug(f"Config Dir  . . . {_config_dir}")
	# logger.debug(f"Data Dir  . . . . {_data_dir}")
	# logger.debug(f"Log Dir . . . . . {_log_dir}")
	# logger.debug(f"Runtime Dir . . . {_runtime_dir}")
	# for d in [_basedir, _cache_dir, _config_dir, _data_dir, _log_dir, _runtime_dir]:
	# 	if not exists(d):
	# 		logger.warning(f"Not Found: {d}")
	# create_tables(_db_rebuild)

	if env.bool("TRUNCATE_TABLES", default=False):
		tables = "dt_price".split()
		with engine.connect() as conn:
			for tbl in tables:
				sql = f"TRUNCATE TABLE {tbl};"
				logger.debug(sql)
				conn.execute(text(sql))
			conn.commit()
	return


def eoj():
	vacuum()
	stop_ts = time.time()
	stop_localtime = time.localtime(stop_ts)
	stop_gmtime = time.gmtime(stop_ts)
	duration = timedelta(seconds=(stop_ts - _run_ts))
	msg = " ".join([
		f"Run Stop : {time.strftime(_iso_datefmt, stop_localtime)}",
		f" Duration: {duration}",
	])
	logger.info(msg)
	return


def do_nothing():
	pass


def create_tables_old(rebuild: bool=False):
	tables = ["dt_price", "dt_daily"]
	for tbl in tables:
		filename = _basedir / "sqlite" / f"{tbl}.sql"
		with engine.connect() as conn, open(filename, "rt") as sqlfile:
			if rebuild:
				logger.debug(f"Dropping table {tbl} . . .")
				sql = f"DROP TABLE IF EXISTS {tbl};"
				conn.execute(text(sql))
			stmnts = sqlfile.read().split(";")
			for sql in stmnts:
				result = conn.execute(text(sql))
				if result.rowcount > 0:
					for row in result.fetchall():
						logger.debug(row)

	return


def extract_prices():
	values = []
	last_ts = -1
	last_date_id = datetime.fromtimestamp(last_ts).date()
	last_log_dict = last_log_entry()
	if last_log_dict:
		last_date_id = last_log_dict["date_id"]
		last_ts = time.mktime(last_date_id.timetuple())
	for entry in [x for x in os.scandir(_log_dir) if x.is_file() and getmtime(x.path) > last_ts]:
		log_dicts = parse_log(entry.path)
		columns = "date_id date_time cad usd".split()
		cols = ",".join(columns)
		placeholders = ",".join([f":{x}" for x in columns])
		for ld in log_dicts:
			params = {
				"date_id": ld["date_id"],
				"date_time": ld["when"],
				"cad": ld["cad"],
				"usd": ld["usd"],
			}
			values.append(params)
		sql = "\n".join([
			f"INSERT INTO dt_price({cols})",
			f"VALUES({placeholders})",
			"ON CONFLICT (date_time) DO NOTHING;",
			# "RETURNING id;"
		])
		if values:
			date_from = log_dicts[0]["date_id"]
			date_thru = log_dicts[-1]["date_id"]
			which_dates = f"{date_from} thru {date_thru}"
			value_count = len(values)
			logger.debug(f"{entry.name} Value Count for {which_dates} . . . {value_count}")
			with engine.connect() as conn:
				# INSERT prices into price table
				result = conn.execute(text(sql), values)
				conn.commit()
			if value_count != result.rowcount:
				logger.debug(f"{entry.name}   Row Count for {which_dates} . . . {result.rowcount}")
			sleep(0.1)
	return


def get_remote_prices():
	host = "vps"
	year_month = _run_dt.strftime("%Y-%m")
	src = f".local/var/log/btc_rates/btc_rates_{year_month}.log"
	dst = os.path.expanduser("~/" + src.replace(".log", "-vps.log"))
	cmd = f"scp -p {host}:{src} {dst}"
	logger.debug(cmd)
	output = run(cmd.split(), capture_output=False)
	if output.returncode != 0:
		logger.warning(output)
	return


def last_log_entry():
	"""
	Find most recent log entry in database
	:return:
	"""
	dt = datetime.fromtimestamp(-1)
	log_dict = {
		"id": -1,
		"date_id": dt.date(),
		"date_time": dt,
		"cad": 0.00,
		"usd": 0.00,
	}
	sql = "SELECT * FROM dt_price ORDER BY date_time DESC LIMIT 1;"
	with engine.connect() as conn:
		try:
			id, date_id, date_time, cad, usd = conn.execute(text(sql)).fetchone()
			log_dict = {"id": id, "date_id": date_id, "date_time": date_time, "cad": cad, "usd": usd}
		except Exception as e:
			logger.warning("No Data")
			do_nothing()
	return log_dict


def parse_log(filename: str | Path) -> list[dict]:
	"""
	Parses BTC Rate logs
	:input: 2024-11-14 05:39:50-0500 btc_rates.get_prices INFO     CAD 127,455.71	USD 90,929.71
	:param filename: name of BTC Rate log file (can have .log or .log.xz extension)
	:return: log_dicts
	"""
	tag = "btc_rates.get_prices"
	log_dicts = []
	open = builtins.open
	if filename.endswith(".xz"):
		open = lzma.open
	with open(filename, "rt") as logfile:
		lines = [x.rstrip() for x in logfile.readlines() if tag in x]
	for i, line in enumerate(lines):
		# logger.info(f"{i + 1:5,d}) {line}")
		parts = line.split()
		if parts[2] != tag:
			do_nothing()
		if len(parts) < 8:
			do_nothing()
		date_id = parts[0]
		when = datetime.fromisoformat(" ".join(parts[0:2]))
		if parts[4] == "CAD":
			cad = float(parts[5].replace(",", ""))
		else:
			continue
		if parts[6] == "USD":
			usd = float(parts[7].replace(",", ""))
		else:
			do_nothing()
		# logger.info(f"       {when} CAD: {cad}  USD: {usd}")
		log_dict = {
			"when": when.isoformat(),
			"date_id": date_id,
			"cad": cad,
			"usd": usd
		}
		log_dicts.append(log_dict)
	return log_dicts


def vacuum():
	sql = "\n".join([
		"SELECT relname, last_vacuum, last_autovacuum, last_analyze, last_autoanalyze",
		"FROM pg_stat_user_tables",
		"WHERE relname = :relname;",
	])
	params = {"relname": "dt_price",}
	# The VACUUM command requires special handling (Ref: https://stackoverflow.com/a/72976667/2719754)
	autocommit_engine = engine.execution_options(isolation_level="AUTOCOMMIT")
	with autocommit_engine.connect() as conn:
		last_vacuum = conn.execute(text(sql), params).fetchone()[1]
		if not last_vacuum:
			last_vacuum = datetime.fromtimestamp(-1).astimezone()
		if (_run_dt - timedelta(days=1)) > last_vacuum:
			logger.debug("Vacuuming . . .")
			result = conn.execute(text("VACUUM ANALYZE;"))
			if result.rowcount > 0:
				for row in result.fetchall():
					logger.debug(row)
				do_nothing()

	return


if __name__ == "__main__":
	_run_ts = time.time()
	_run_dt = datetime.fromtimestamp(_run_ts).astimezone()
	_run_localtime = time.localtime(_run_ts)
	_run_gmtime = time.gmtime(_run_ts)
	_run_ymd = time.strftime("%Y%m%d", _run_localtime)
	_run_hms = time.strftime("%H%M%S", _run_localtime)
	_run_ymdhms = f"{_run_ymd}_{_run_hms}"
	_iso_datefmt = "%Y-%m-%d %H:%M:%S%z"

	# Environment
	_db_rebuild = env.bool("DB_REBUILD", default=False)
	_debug = env.bool("DEBUG", default=False)

	# Directories
	_cache_dir = Path(XDG_CACHE_HOME) / __project__
	_config_dir = Path(XDG_CONFIG_HOME) / __project__
	_data_dir = Path(XDG_DATA_HOME) / __project__
	_log_dir = Path(XDG_DATA_HOME).parent / "var" / "log" / __project__
	_runtime_dir = Path(XDG_RUNTIME_DIR) / __project__

	# Sub-Directories
	_log_dir2 = _cache_dir / "log"

	# Configure Logging (using Loguru)
	_logfile = _log_dir2 / f"{__module__}.log"
	_errfile = _log_dir2 / f"{__module__}.err"

	logger.remove(0)
	logger.add(
		sys.stderr,
		backtrace=True,
		colorize=True,
		diagnose=True,
		format="<level>{level:8s} {function}:{line:03d}  {message}</level>",
		level=0,
	)
	logger.add(
		_logfile,
		colorize=False,
		compression="gz",
		format="<green>{time}</green> <level>{level:8s} {name}:{function}:{line:03d}  {message}</level>",
		rotation="10 MB",
		level="TRACE",
	)
	logger.add(
		_errfile,
		colorize=False,
		compression="gz",
		format="<green>{time}></green> <level>{level:8s} {name}:{function}:{line:03d}  {message}</level>",
		rotation="10 MB",
		level="ERROR",
	)
	# logger.add("debug.log", rotation="10 MB", level="DEBUG")
	# logger.add("info.log", rotation="10 MB", level="INFO")
	# logger.add("warning.log", rotation="10 MB", level="WARNING")
	# logger.add("critical.log", rotation="10 MB", level="CRITICAL")

	# Not sure how to add PostgreSQL notice
	# import logging
	# logging.basicConfig()  # log messages to stdout
	# logging.getLogger('sqlalchemy.dialects.postgresql').setLevel(logging.INFO)

	# Database
	# _db_url = "sqlite:///" + str(_basedir / env("SQLITE_DB_NAME"))
	_db_url = env("SQLALCHEMY_DATABASE_URI")
	_db_schema = env("DB_SCHEMA")
	engine = create_engine(_db_url, echo=_debug)

	@sa.event.listens_for(engine, "connect", insert=True)
	def set_search_path(dbapi_connection, connection_record):
		"""
		Set schema search path in database
		"""
		sql = f"SET SESSION search_path TO {_db_schema},public;"
		existing_autocommit = dbapi_connection.autocommit
		dbapi_connection.autocommit = True
		cursor = dbapi_connection.cursor()
		cursor.execute(sql)
		cursor.close()
		dbapi_connection.autocommit = existing_autocommit

	# with engine.connect() as conn:
	# 	result = conn.execute(text("SELECT * FROM dt_price LIMIT 10;"))
	# 	if result.rowcount:
	# 		for row in result.fetchall():
	# 			logger.info(row)
	# sys.exit()

	init()
	main()
	eoj()
