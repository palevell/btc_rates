#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# price_extractor - Thursday, November 14, 2024
""" Extract & analyze timestamps and prices from btc_rate logs """
__version__ = "0.4.4-dev0"

import builtins
import lzma
import os
import sys
import time
from datetime import datetime, timedelta
from glob import glob
from os.path import basename, exists, getmtime, join, lexists, realpath
from pathlib import Path
from time import sleep

import sqlalchemy as sa
import xdg
from loguru import logger
from environs import Env
from sqlalchemy import create_engine, text
from subprocess import run
# from xdg import XDG_CACHE_HOME, XDG_CONFIG_HOME, XDG_DATA_HOME, XDG_RUNTIME_DIR

_basedir = Path(__file__).resolve().parent.parent
__org__ = "LevellTech"
__project__ = _basedir.stem
__module__ = Path(__file__).resolve().stem

env = Env(expand_vars=True)
env.read_env()


def main():
	get_remote_prices()
	extract_prices()
	return


def init():
	started = time.strftime(_iso_datefmt, _run_localtime)
	logger.info(f"Run Start: {__project__}.{__module__} v{__version__} {started}")
	# for d in [_cache_dir, _config_dir, _data_dir, _log_dir, _runtime_dir]:
	# 	os.makedirs(d, exist_ok=True)
	# logger.debug(f"Organization  . . {__org__}")
	# logger.debug(f"Project . . . . . {__project__}")
	# logger.debug(f"Module  . . . . . {__module__}")
	# logger.debug(f"Base Dir  . . . . {_basedir}")
	# logger.debug(f"Cache Dir . . . . {_cache_dir}")
	# logger.debug(f"Config Dir  . . . {_config_dir}")
	# logger.debug(f"Data Dir  . . . . {_data_dir}")
	# logger.debug(f"Log Dir . . . . . {_log_dir}")
	# logger.debug(f"Runtime Dir . . . {_runtime_dir}")
	# for d in [_basedir, _cache_dir, _config_dir, _data_dir, _log_dir, _runtime_dir]:
	# 	if not exists(d):
	# 		logger.warning(f"Not Found: {d}")
	# create_tables(_db_rebuild)

	if env.bool("TRUNCATE_TABLES", default=False):
		tables = "dt_price".split()
		with engine.connect() as conn:
			for tbl in tables:
				sql = f"TRUNCATE TABLE {tbl};"
				logger.debug(sql)
				conn.execute(text(sql))
			conn.commit()
	return


def eoj():
	# vacuum()
	stop_ts = time.time()
	stop_localtime = time.localtime(stop_ts)
	stop_gmtime = time.gmtime(stop_ts)
	duration = timedelta(seconds=(stop_ts - _run_ts))
	msg = " ".join([
		f"Run Stop : {time.strftime(_iso_datefmt, stop_localtime)}",
		f" Duration: {duration}",
	])
	logger.info(msg)
	return


def do_nothing():
	pass


def create_tables_old(rebuild: bool=False):
	tables = ["dt_price", "dt_daily"]
	for tbl in tables:
		filename = _basedir / "sqlite" / f"{tbl}.sql"
		with engine.connect() as conn, open(filename, "rt") as sqlfile:
			if rebuild:
				logger.debug(f"Dropping table {tbl} . . .")
				sql = f"DROP TABLE IF EXISTS {tbl};"
				conn.execute(text(sql))
			stmnts = sqlfile.read().split(";")
			for sql in stmnts:
				result = conn.execute(text(sql))
				if result.rowcount > 0:
					for row in result.fetchall():
						logger.debug(row)

	return


def extract_prices():
	values = []
	last_ts = -1
	last_date_id = datetime.fromtimestamp(last_ts).date()
	last_log_dict = last_log_entry()
	if last_log_dict:
		# Subtract one day from date of last price in database (to accommodate late-arriving files)
		last_date_id = (last_log_dict["date_id"] - timedelta(days=1))
		last_ts = time.mktime(last_date_id.timetuple())
	for entry in [x for x in os.scandir(_log_dir) if x.name.startswith("btc_rates_") and ".log" in x.name and getmtime(x.path) > last_ts]:
		log_dicts = parse_log(entry.path)
		if not log_dicts:
			continue
		columns = "date_id date_time cad usd".split()
		cols = ",".join(columns)
		placeholders = ",".join([f":{x}" for x in columns])
		for ld in log_dicts:
			params = {
				"date_id": ld["date_id"],
				"date_time": ld["when"],
				"cad": ld["cad"],
				"usd": ld["usd"],
			}
			values.append(params)
		sql = "\n".join([
			f"INSERT INTO dt_price({cols})",
			f"VALUES({placeholders})",
			"ON CONFLICT (date_time) DO NOTHING;",
			# "RETURNING id;"
		])
		if values:
			date_from = log_dicts[0]["date_id"]
			date_thru = log_dicts[-1]["date_id"]
			which_dates = f"{date_from} thru {date_thru}"
			value_count = len(values)
			logger.debug(f"{entry.name:30s} Value Count for {which_dates} . . . {value_count}")
			with engine.connect() as conn:
				# INSERT prices into price table
				result = conn.execute(text(sql), values)
				conn.commit()
			if value_count != result.rowcount:
				logger.debug(f"{entry.name:30s}   Row Count for {which_dates} . . . {result.rowcount}")
			sleep(0.1)
	return


def get_remote_prices():
	host = "vps"
	# ToDo: Update this to the XDG_STATE_HOME directory
	# .local/state/LevellTech/btc_rates/log (_log_dir)
	vps_dir = ".local/var/log/btc_rates"
	cmd = f"ssh {host} find {vps_dir} -mtime -1 -type f"
	output = run(cmd.split(), capture_output=True)
	if output.returncode != 0:
		logger.warning(output.stderr)
		do_nothing()
	filenames = output.stdout.decode("utf8").splitlines()
	for src in filenames:
		# src = f".local/var/log/btc_rates/btc_rates_{filename}.log"
		# dst = os.path.expanduser("~/" + src.replace(".log", "-vps.log"))
		dst = _log_dir / basename(src).replace(".log", f"-{host}.log")
		cmd = f"scp -p {host}:{src} {dst}"
		logger.debug(cmd)
		output = run(cmd.split(), capture_output=False)
		if output.returncode != 0:
			logger.warning(output.stderr)
			do_nothing()
	return


def last_log_entry():
	"""
	Find most recent log entry in database
	:return:
	"""
	dt = datetime.fromtimestamp(-1)
	log_dict = {
		"id": -1,
		"date_id": dt.date(),
		"date_time": dt,
		"cad": 0.00,
		"usd": 0.00,
	}
	sql = "SELECT * FROM dt_price ORDER BY date_time DESC LIMIT 1;"
	with engine.connect() as conn:
		try:
			id, date_id, date_time, cad, usd = conn.execute(text(sql)).fetchone()
			log_dict = {"id": id, "date_id": date_id, "date_time": date_time, "cad": cad, "usd": usd}
		except Exception as e:
			logger.warning("No Data")
			do_nothing()
	return log_dict


def parse_log(filename: str | Path) -> list[dict]:
	"""
	Parses BTC Rate logs
	:input: 2024-11-14 05:39:50-0500 btc_rates.get_prices INFO     CAD 127,455.71	USD 90,929.71
	:param filename: name of BTC Rate log file (can have .log or .log.xz extension)
	:return: log_dicts
	"""
	tag = "btc_rates.get_prices"
	log_dicts = []
	open = builtins.open
	if filename.endswith(".xz"):
		open = lzma.open
	with open(filename, "rt") as logfile:
		lines = [x.rstrip() for x in logfile.readlines() if tag in x]
	if "btc_rates3" in filename:
		do_nothing()
	for i, line in enumerate(lines):
		# logger.info(f"{i + 1:5,d}) {line}")
		parts = line.split()
		if parts[3] != "INFO":
			continue
		if parts[2] != tag:
			continue
		if len(parts) < 8:
			do_nothing()
		date_id = parts[0]
		when = datetime.fromisoformat(" ".join(parts[0:2]))
		if parts[4] == "CAD":
			cad = float(parts[5].replace(",", ""))
		else:
			continue
		if parts[6] == "USD":
			usd = float(parts[7].replace(",", ""))
		else:
			do_nothing()
		# logger.info(f"       {when} CAD: {cad}  USD: {usd}")
		log_dict = {
			"when": when.isoformat(),
			"date_id": date_id,
			"cad": cad,
			"usd": usd
		}
		log_dicts.append(log_dict)
	return log_dicts


def vacuum():
	sql = "\n".join([
		"SELECT relname, last_vacuum, last_autovacuum, last_analyze, last_autoanalyze",
		"FROM pg_stat_user_tables",
		"WHERE relname = :relname;",
	])
	params = {"relname": "dt_price",}
	# The VACUUM command requires special handling (Ref: https://stackoverflow.com/a/72976667/2719754)
	autocommit_engine = engine.execution_options(isolation_level="AUTOCOMMIT")
	with autocommit_engine.connect() as conn:
		last_vacuum = conn.execute(text(sql), params).fetchone()[1]
		if not last_vacuum:
			last_vacuum = datetime.fromtimestamp(-1).astimezone()
		if (_run_dt - timedelta(days=1)) > last_vacuum:
			logger.debug("Vacuuming . . .")
			result = conn.execute(text("VACUUM ANALYZE;"))
			if result.rowcount > 0:
				for row in result.fetchall():
					logger.debug(row)
				do_nothing()

	return


if __name__ == "__main__":
	_run_ts = time.time()
	_run_dt = datetime.fromtimestamp(_run_ts).astimezone()
	_run_localtime = time.localtime(_run_ts)
	_run_gmtime = time.gmtime(_run_ts)
	_run_ymd = time.strftime("%Y%m%d", _run_localtime)
	_run_hms = time.strftime("%H%M%S", _run_localtime)
	_run_ymdhms = f"{_run_ymd}_{_run_hms}"
	_iso_datefmt = "%Y-%m-%d %H:%M:%S%z"

	# Environment
	_db_rebuild = env.bool("DB_REBUILD", default=False)
	_debug = env.bool("DEBUG", default=False)

	# Directories
	_cache_dir = xdg.xdg_cache_home() / __org__ / __project__
	_config_dir = xdg.xdg_config_home() / __org__ / __project__
	_data_dir = xdg.xdg_data_home() / __org__ / __project__
	_runtime_dir = xdg.xdg_runtime_dir() / __org__ / __project__
	_state_dir = xdg.xdg_state_home() / __org__ / __project__
	# Sub-Directories
	_log_dir = _state_dir / "log"

	# Sub-Directories
	# _log_dir2 = _cache_dir / "log"

	# Configure Logging (using Loguru)
	_logfile = _log_dir / f"{__module__}.log"
	_errfile = _log_dir / f"{__module__}.err"

	logger.remove(0)
	logger.add(
		sys.stderr,
		backtrace=True,
		colorize=True,
		diagnose=True,
		format="<level>{level:8s} {function}:{line:03d}  {message}</level>",
		level=0,
	)
	logger.add(
		_logfile,
		colorize=False,
		compression="gz",
		format="<green>{time}</green> <level>{level:8s} {name}:{function}:{line:03d}  {message}</level>",
		rotation="10 MB",
		level="TRACE",
	)
	logger.add(
		_errfile,
		colorize=False,
		compression="gz",
		format="<green>{time}></green> <level>{level:8s} {name}:{function}:{line:03d}  {message}</level>",
		rotation="10 MB",
		level="ERROR",
	)
	# logger.add("debug.log", rotation="10 MB", level="DEBUG")
	# logger.add("info.log", rotation="10 MB", level="INFO")
	# logger.add("warning.log", rotation="10 MB", level="WARNING")
	# logger.add("critical.log", rotation="10 MB", level="CRITICAL")

	# Database
	_db_url = env("SQLALCHEMY_DATABASE_URI")
	# _db_url = "sqlite:///" + str(_basedir / env("SQLITE_DB_NAME"))
	_db_schema = env("DB_SCHEMA")
	engine = create_engine(_db_url, echo=_debug)

	@sa.event.listens_for(engine, "connect", insert=True)
	def set_search_path(dbapi_connection, connection_record):
		"""
		Set schema search path in database
		"""
		sql = f"SET SESSION search_path TO {_db_schema},public;"
		existing_autocommit = dbapi_connection.autocommit
		dbapi_connection.autocommit = True
		cursor = dbapi_connection.cursor()
		cursor.execute(sql)
		cursor.close()
		dbapi_connection.autocommit = existing_autocommit

	# with engine.connect() as conn:
	# 	result = conn.execute(text("SELECT * FROM dt_price LIMIT 10;"))
	# 	if result.rowcount:
	# 		for row in result.fetchall():
	# 			logger.info(row)
	# sys.exit()

	init()
	main()
	eoj()
